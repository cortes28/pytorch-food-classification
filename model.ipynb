{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4409cee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from   torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f285e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rdm():\n",
    "    \"\"\"\n",
    "    This is a test function you can call to see if u have imported all from model.ipynb if you \n",
    "    called as import 'from model.ipynb import *'. Note that there may be function code errors which may require more\n",
    "    diagnosing. \n",
    "    \"\"\"\n",
    "    print(\"You were able to access a function within `model.ipynb`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbfdda",
   "metadata": {},
   "source": [
    "# Steps for the dataloader to either train or test model\n",
    "* train_epoch() trains the model on that epoch on the dataloader given\n",
    "* test_epoch() tests the model on that epoch on the dataloader given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30627fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a step for each train epoch\n",
    "def train_epoch(model:torch.nn.Module,\n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                device=device):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a train step for an epoch during its training process. Within this train step, we will iterate through a model in\n",
    "    training mode with the training data in batches. \n",
    "    \"\"\"\n",
    "    \n",
    "    # put it in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy that we will calculate per epoch \n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # loop through dataloader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 1. forward pass which outputs model logits\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # 2. loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # 3. optimize the zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. loss backwards\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate the accuracy metric for the current batch (y_pred_class is the predicted value)\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
    "\n",
    "        \n",
    "    # adjust metric accuracy loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6907a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test epoch\n",
    "def test_epoch(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               device=device):\n",
    "    \"\"\"\n",
    "    This is a test step for an epoch during its testing process. Within this train step, we will iterate through a model in\n",
    "    test (eval) mode with the testing data in batches. \n",
    "    \"\"\"\n",
    "    # put in evaluation mode (to not modify any parameters of the model)\n",
    "    model.eval()\n",
    "    \n",
    "    # setup the test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # turn on inference model for testing\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        # lop through the dataloader batch\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            # send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. forward pass\n",
    "            test_pred_logits = model(X)\n",
    "            \n",
    "            # 2. calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # calculate accuracy for X, y batch\n",
    "            test_pred_label = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_label == y).sum().item()/len(test_pred_label))\n",
    "            \n",
    "        # adjust metric to get the average of both test loss and accuracy\n",
    "        \n",
    "        test_loss = test_loss / len(dataloader)\n",
    "        test_acc = test_acc / len(dataloader)\n",
    "        \n",
    "    return test_loss, test_acc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fbcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train function  that takes in various parameters (needs model, optimizer, loss, dataloader)\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 20,\n",
    "          device=device,\n",
    "          display_epoch: bool = True):\n",
    "\n",
    "    from tqdm.auto import tqdm    \n",
    "\n",
    "    # empty dictionary for our results\n",
    "    results = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    # loop through the training and testing epochs for in range(epochs) (default 20)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_epoch(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           optimizer=optimizer,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           device=device\n",
    "                                           )\n",
    "        test_loss, test_acc = test_epoch(model=model,\n",
    "                                         dataloader=test_dataloader,\n",
    "                                         loss_fn=loss_fn,\n",
    "                                         device=device)\n",
    "        \n",
    "        # print out information regarding model results per epoch (default set to true)\n",
    "        if display_epoch:\n",
    "           print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\") \n",
    "        \n",
    "        # update the results within the dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b00645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A food classifier module that follows the architecture of a CNN + modificaitons that I added.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        input_shape - our case will be 3 when we input due to the shape of x\n",
    "        hidden_units - amount of intermediate layers between the input and output layer.\n",
    "        output_shape - amount of classes that we got (will be set to the len of classes)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #\n",
    "        #\n",
    "        # will block it up for convenience sake\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=4,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=4,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,\n",
    "                        stride=3)  # Note that the default is the same as kernel_size \n",
    "\n",
    "        )\n",
    "        #\n",
    "        #\n",
    "        # Block 2\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=4,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=4,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,\n",
    "                        stride=3)  # Note that the default is the same as kernel_size \n",
    "\n",
    "        )\n",
    "        #\n",
    "        #\n",
    "        # classifier\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units * 1 * 1,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        print(f\"shape of x: {x.shape}\")\n",
    "        x = self.block_2(x)\n",
    "        print(f\"shape after 2nd block {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        print(f\"after classifier: {x.shape}\")\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dfa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575132c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
